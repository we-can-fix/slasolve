#!/bin/bash
# ============================================================================
# å‚™ä»½è…³æœ¬ (Backup Script)
# ============================================================================
# åŸ·è¡Œå®Œæ•´æˆ–å¢é‡å‚™ä»½ï¼Œæ”¯æŒå¤šç¨®å‚™ä»½ç›®æ¨™
# ============================================================================

set -euo pipefail

# é¡è‰²å®šç¾©
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# é…ç½®
BACKUP_TYPE="${1:-full}"  # full, incremental, database
BACKUP_DIR="${BACKUP_DIR:-/tmp/backups}"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
BACKUP_NAME="slasolve_${BACKUP_TYPE}_${TIMESTAMP}"
RETENTION_DAYS=30

# S3 é…ç½® (å¦‚æœä½¿ç”¨)
S3_BUCKET="${S3_BUCKET:-slasolve-backups}"
S3_REGION="${S3_REGION:-us-east-1}"

# æ—¥èªŒå‡½æ•¸
log() {
    echo -e "${BLUE}[$(date +'%Y-%m-%d %H:%M:%S')]${NC} $1"
}

error() {
    echo -e "${RED}[ERROR]${NC} $1" >&2
}

success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

# æª¢æŸ¥ä¾è³´
check_dependencies() {
    local deps=("tar" "gzip" "sha256sum")
    
    for cmd in "${deps[@]}"; do
        if ! command -v "$cmd" &> /dev/null; then
            error "Required command not found: $cmd"
            exit 1
        fi
    done
    
    # æª¢æŸ¥ AWS CLI (å¦‚æœéœ€è¦)
    if [ -n "${S3_BUCKET:-}" ]; then
        if ! command -v aws &> /dev/null; then
            warning "AWS CLI not found, skipping S3 upload"
            S3_BUCKET=""
        fi
    fi
}

# å‰µå»ºå‚™ä»½ç›®éŒ„
prepare_backup_dir() {
    log "æº–å‚™å‚™ä»½ç›®éŒ„: $BACKUP_DIR"
    mkdir -p "$BACKUP_DIR"
    
    local backup_path="$BACKUP_DIR/$BACKUP_NAME"
    mkdir -p "$backup_path"
    echo "$backup_path"
}

# å‚™ä»½é…ç½®æ–‡ä»¶
backup_config() {
    local backup_path="$1"
    log "å‚™ä»½é…ç½®æ–‡ä»¶..."
    
    local config_files=(
        "docker-compose.yml"
        "docker-compose.prod.yml"
        ".env.production"
        "auto-fix-bot.yml"
        "nginx.conf"
    )
    
    mkdir -p "$backup_path/config"
    
    for file in "${config_files[@]}"; do
        if [ -f "$file" ]; then
            cp "$file" "$backup_path/config/"
            log "  âœ“ $file"
        fi
    done
    
    # å‚™ä»½ Kubernetes é…ç½®
    if [ -d "k8s" ]; then
        cp -r k8s "$backup_path/config/"
        log "  âœ“ k8s/"
    fi
}

# å‚™ä»½ä»£ç¢¼
backup_code() {
    local backup_path="$1"
    log "å‚™ä»½ä»£ç¢¼..."
    
    local dirs=(
        "core"
        "mcp-servers"
        "advanced-system-src"
        "scripts"
    )
    
    mkdir -p "$backup_path/code"
    
    for dir in "${dirs[@]}"; do
        if [ -d "$dir" ]; then
            tar -czf "$backup_path/code/${dir}.tar.gz" "$dir"
            log "  âœ“ $dir"
        fi
    done
}

# å‚™ä»½æ•¸æ“šåº«
backup_database() {
    local backup_path="$1"
    log "å‚™ä»½æ•¸æ“šåº«..."
    
    mkdir -p "$backup_path/database"
    
    # PostgreSQL å‚™ä»½ (å¦‚æœä½¿ç”¨)
    if [ -n "${POSTGRES_HOST:-}" ]; then
        local db_file="$backup_path/database/postgres_${TIMESTAMP}.sql.gz"
        
        PGPASSWORD="${POSTGRES_PASSWORD}" pg_dump \
            -h "${POSTGRES_HOST}" \
            -U "${POSTGRES_USER}" \
            -d "${POSTGRES_DB}" \
            | gzip > "$db_file"
        
        log "  âœ“ PostgreSQL å‚™ä»½å®Œæˆ"
    fi
    
    # Redis å‚™ä»½ (å¦‚æœä½¿ç”¨)
    if [ -n "${REDIS_HOST:-}" ]; then
        local redis_file="$backup_path/database/redis_${TIMESTAMP}.rdb"
        
        redis-cli -h "${REDIS_HOST}" --rdb "$redis_file"
        log "  âœ“ Redis å‚™ä»½å®Œæˆ"
    fi
}

# å‚™ä»½æ•¸æ“šç›®éŒ„
backup_data() {
    local backup_path="$1"
    log "å‚™ä»½æ•¸æ“šç›®éŒ„..."
    
    local data_dirs=(
        "/data/provenance"
        "/data/slsa"
        "/data/logs"
    )
    
    mkdir -p "$backup_path/data"
    
    for dir in "${data_dirs[@]}"; do
        if [ -d "$dir" ]; then
            local dir_name=$(basename "$dir")
            tar -czf "$backup_path/data/${dir_name}.tar.gz" "$dir"
            log "  âœ“ $dir"
        fi
    done
}

# ç”Ÿæˆå‚™ä»½æ¸…å–®
generate_manifest() {
    local backup_path="$1"
    log "ç”Ÿæˆå‚™ä»½æ¸…å–®..."
    
    cat > "$backup_path/manifest.json" <<EOF
{
  "backup_name": "$BACKUP_NAME",
  "backup_type": "$BACKUP_TYPE",
  "timestamp": "$TIMESTAMP",
  "hostname": "$(hostname)",
  "version": "1.0.0",
  "files": []
}
EOF
    
    # ç”Ÿæˆæ–‡ä»¶åˆ—è¡¨å’Œæ ¡é©—å’Œ
    find "$backup_path" -type f ! -name "manifest.json" ! -name "*.sha256" -exec sha256sum {} \; > "$backup_path/checksums.sha256"
    
    log "  âœ“ æ¸…å–®å·²ç”Ÿæˆ"
}

# å£“ç¸®å‚™ä»½
compress_backup() {
    local backup_path="$1"
    log "å£“ç¸®å‚™ä»½..."
    
    local archive_path="${backup_path}.tar.gz"
    tar -czf "$archive_path" -C "$BACKUP_DIR" "$BACKUP_NAME"
    
    # ç”Ÿæˆæ ¡é©—å’Œ
    sha256sum "$archive_path" > "${archive_path}.sha256"
    
    # åˆªé™¤è‡¨æ™‚ç›®éŒ„
    rm -rf "$backup_path"
    
    local size=$(du -h "$archive_path" | cut -f1)
    success "å‚™ä»½å£“ç¸®å®Œæˆ: $archive_path ($size)"
    
    echo "$archive_path"
}

# ä¸Šå‚³åˆ° S3
upload_to_s3() {
    local archive_path="$1"
    
    if [ -z "${S3_BUCKET:-}" ]; then
        return 0
    fi
    
    log "ä¸Šå‚³å‚™ä»½åˆ° S3..."
    
    aws s3 cp "$archive_path" "s3://$S3_BUCKET/backups/" \
        --region "$S3_REGION" \
        --storage-class STANDARD_IA
    
    aws s3 cp "${archive_path}.sha256" "s3://$S3_BUCKET/backups/" \
        --region "$S3_REGION"
    
    success "å‚™ä»½å·²ä¸Šå‚³åˆ° S3: s3://$S3_BUCKET/backups/$(basename $archive_path)"
}

# æ¸…ç†èˆŠå‚™ä»½
cleanup_old_backups() {
    log "æ¸…ç† $RETENTION_DAYS å¤©å‰çš„èˆŠå‚™ä»½..."
    
    find "$BACKUP_DIR" -name "slasolve_*.tar.gz" -mtime +$RETENTION_DAYS -delete
    find "$BACKUP_DIR" -name "slasolve_*.tar.gz.sha256" -mtime +$RETENTION_DAYS -delete
    
    # S3 æ¸…ç† (å¦‚æœä½¿ç”¨)
    if [ -n "${S3_BUCKET:-}" ]; then
        aws s3 ls "s3://$S3_BUCKET/backups/" | while read -r line; do
            createDate=$(echo "$line" | awk '{print $1" "$2}')
            createDate=$(date -d "$createDate" +%s)
            olderThan=$(date -d "$RETENTION_DAYS days ago" +%s)
            
            if [[ $createDate -lt $olderThan ]]; then
                fileName=$(echo "$line" | awk '{print $4}')
                if [[ $fileName != "" ]]; then
                    aws s3 rm "s3://$S3_BUCKET/backups/$fileName"
                    log "  âœ“ åˆªé™¤: $fileName"
                fi
            fi
        done
    fi
    
    success "èˆŠå‚™ä»½æ¸…ç†å®Œæˆ"
}

# å®Œæ•´å‚™ä»½
full_backup() {
    log "é–‹å§‹å®Œæ•´å‚™ä»½..."
    
    local backup_path=$(prepare_backup_dir)
    
    backup_config "$backup_path"
    backup_code "$backup_path"
    backup_database "$backup_path"
    backup_data "$backup_path"
    generate_manifest "$backup_path"
    
    local archive_path=$(compress_backup "$backup_path")
    upload_to_s3 "$archive_path"
    
    success "å®Œæ•´å‚™ä»½å®Œæˆ: $archive_path"
}

# å¢é‡å‚™ä»½
incremental_backup() {
    log "é–‹å§‹å¢é‡å‚™ä»½..."
    
    local backup_path=$(prepare_backup_dir)
    
    # åªå‚™ä»½æœ€è¿‘ 24 å°æ™‚å…§ä¿®æ”¹çš„æ–‡ä»¶
    backup_config "$backup_path"
    backup_database "$backup_path"
    
    # æ•¸æ“šç›®éŒ„å¢é‡å‚™ä»½
    mkdir -p "$backup_path/data"
    find /data -type f -mtime -1 -exec cp --parents {} "$backup_path/data/" \;
    
    generate_manifest "$backup_path"
    
    local archive_path=$(compress_backup "$backup_path")
    upload_to_s3 "$archive_path"
    
    success "å¢é‡å‚™ä»½å®Œæˆ: $archive_path"
}

# åƒ…æ•¸æ“šåº«å‚™ä»½
database_only_backup() {
    log "é–‹å§‹æ•¸æ“šåº«å‚™ä»½..."
    
    local backup_path=$(prepare_backup_dir)
    
    backup_database "$backup_path"
    generate_manifest "$backup_path"
    
    local archive_path=$(compress_backup "$backup_path")
    upload_to_s3 "$archive_path"
    
    success "æ•¸æ“šåº«å‚™ä»½å®Œæˆ: $archive_path"
}

# ä¸»å‡½æ•¸
main() {
    echo "============================================================================"
    echo "SLASolve å‚™ä»½ç³»çµ±"
    echo "============================================================================"
    echo ""
    
    log "å‚™ä»½é¡å‹: $BACKUP_TYPE"
    log "å‚™ä»½ç›®éŒ„: $BACKUP_DIR"
    log "æ™‚é–“æˆ³: $TIMESTAMP"
    echo ""
    
    check_dependencies
    
    case "$BACKUP_TYPE" in
        full)
            full_backup
            ;;
        incremental)
            incremental_backup
            ;;
        database)
            database_only_backup
            ;;
        *)
            error "æœªçŸ¥çš„å‚™ä»½é¡å‹: $BACKUP_TYPE"
            echo "ä½¿ç”¨æ–¹å¼: $0 [full|incremental|database]"
            exit 1
            ;;
    esac
    
    cleanup_old_backups
    
    echo ""
    success "ğŸ‰ å‚™ä»½ä»»å‹™å®Œæˆï¼"
}

# åŸ·è¡Œä¸»å‡½æ•¸
main
